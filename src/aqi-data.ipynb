{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US EPA Air Quality System API Data Acquisition\n",
    "\n",
    "This notebook serves as a comprehensive guide to accessing air quality data from the US Environmental Protection Agency (EPA) Air Quality Service (AQS) API. It outlines various techniques employed to pinpoint the air quality monitoring station in proximity to Muskogee, Oklahoma, which is essential for accurate data retrieval.\n",
    "\n",
    "Throughout the notebook, you'll find a series of methods and approaches tested to identify the most suitable station for data collection. After exploring different strategies, the notebook ultimately settles on leveraging monthly estimates for the Air Quality Index (AQI). This method ensures reliable and consistent data acquisition, allowing for informed insights into air quality trends and conditions in the Muskogee, Oklahoma area.\n",
    "\n",
    "## License\n",
    "This code snippets are developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.1 - September 5, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- importing necessary libraries ---------------------- #\n",
    "\n",
    "import pandas as pd\n",
    "import json, time\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- defining constants ---------------------------- #\n",
    "\n",
    "API_REQUEST_URL = 'https://aqs.epa.gov/data/api'\n",
    "\n",
    "API_ACTION_SIGNUP = '/signup?email={email}'\n",
    "\n",
    "# List actions provide information on API parameter values that are required by some other actions/requests\n",
    "API_ACTION_LIST_CLASSES = '/list/classes?email={email}&key={key}'\n",
    "API_ACTION_LIST_PARAMS = '/list/parametersByClass?email={email}&key={key}&pc={pclass}'\n",
    "API_ACTION_LIST_SITES = '/list/sitesByCounty?email={email}&key={key}&state={state}&county={county}'\n",
    "\n",
    "# Monitor actions are requests for monitoring stations that meet specific criteria\n",
    "API_ACTION_MONITORS_COUNTY = '/monitors/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_MONITORS_BOX = '/monitors/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "\n",
    "# Summary actions are requests for summary data. These are for daily summaries\n",
    "API_ACTION_DAILY_SUMMARY_COUNTY = '/dailyData/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_DAILY_SUMMARY_BOX = '/dailyData/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "\n",
    "# It is always nice to be respectful of a free data resource.\n",
    "# We're going to observe a 100 requests per minute limit - which is fairly nice\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# This is a template that covers most of the parameters for the actions we might take, from the set of actions\n",
    "# above. In the examples below, most of the time parameters can either be supplied as individual values to a\n",
    "# function - or they can be set in a copy of the template and passed in with the template.\n",
    "\n",
    "AQS_REQUEST_TEMPLATE = {\n",
    "    \"email\":      \"\",\n",
    "    \"key\":        \"\",\n",
    "    \"state\":      \"\",     # the two digit state FIPS # as a string\n",
    "    \"county\":     \"\",     # the three digit county FIPS # as a string\n",
    "    \"begin_date\": \"\",     # the start of a time window in YYYYMMDD format\n",
    "    \"end_date\":   \"\",     # the end of a time window in YYYYMMDD format, begin_date and end_date must be in the same year\n",
    "    \"minlat\":    0.0,\n",
    "    \"maxlat\":    0.0,\n",
    "    \"minlon\":    0.0,\n",
    "    \"maxlon\":    0.0,\n",
    "    \"param\":     \"\",     # a list of comma separated 5 digit codes, max 5 codes requested\n",
    "    \"pclass\":    \"\"      # parameter class is only used by the List calls\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used to create API Keys to access the data. Once we have the API keys, we comment out the code to avoid running it and generating a new key again. This practice helps maintain the security and integrity of the existing API keys and ensures that they are not accidentally overwritten or exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# #    This implements the sign-up request. The parameters are standardized so that this function definition matches\n",
    "# #    all of the others. However, the easiest way to call this is to simply call this function with your preferred\n",
    "# #    email address.\n",
    "# #\n",
    "def request_signup(email_address = None,\n",
    "                   endpoint_url = API_REQUEST_URL,\n",
    "                   endpoint_action = API_ACTION_SIGNUP,\n",
    "                   request_template = AQS_REQUEST_TEMPLATE,\n",
    "                   headers = None):\n",
    "\n",
    "    # Make sure we have a string - if you don't have access to this email addres, things might go badly for you\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_signup()'\")\n",
    "\n",
    "    # Compose the signup url - create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "# #\n",
    "# #    A SIGNUP request is only to be done once, to request a key. A key is sent to that email address and needs to be confirmed with a click through\n",
    "# #    This code should probably be commented out after you've made your key request to make sure you don't accidentally make a new sign-up request\n",
    "# #\n",
    "# print(\"Requesting SIGNUP ...\")\n",
    "# response = request_signup(\"nsaumya@uw.edu\")\n",
    "# print(json.dumps(response,indent=4))\n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure accurate air quality monitoring, it's crucial to understand the various types of air quality sensors and the diverse locations where air quality stations are deployed. Different sensor technologies, such as optical, chemical, and particulate sensors, provide insights into various air pollutants, including PM2.5, PM10, VOCs, CO, NO2, and more. Air quality stations are strategically placed in urban areas, industrial zones, traffic intersections, and even residential neighborhoods to capture a wide range of air quality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'nsaumya@uw.edu'\n",
    "APIKEY = 'bolegazelle84'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_list_info(email_address=None, key=None,\n",
    "                      endpoint_url=API_REQUEST_URL,\n",
    "                      endpoint_action=API_ACTION_LIST_CLASSES,\n",
    "                      request_template=AQS_REQUEST_TEMPLATE,\n",
    "                      headers=None):\n",
    "    \"\"\"\n",
    "    Requests a list of information using the EPA Air Quality Service (AQS) API.\n",
    "\n",
    "    Parameters:\n",
    "    email_address (str): The email address associated with the API access.\n",
    "    key (str): The API key for authentication.\n",
    "    endpoint_url (str): The base URL for API requests.\n",
    "    endpoint_action (str): The specific API action to perform.\n",
    "    request_template (dict): The request template with parameters to be filled.\n",
    "    headers (dict): Optional headers for the HTTP request.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the API response in JSON format.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If email_address or key is missing in the request_template.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure we have email and key - at least\n",
    "    # this prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "\n",
    "    # for the basic request, we need an email address and a key\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_list_info()'\")\n",
    "    if not request_template['key']:\n",
    "        raise Exception(\"Must supply a key to call 'request_list_info()'\")\n",
    "\n",
    "    # compose the request URL\n",
    "    request_url = endpoint_url + endpoint_action.format(**request_template)\n",
    "\n",
    "    try:\n",
    "        # wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"AIRNOW MAPS\",\n",
      "        \"value_represented\": \"The parameters represented on AirNow maps (88101, 88502, and 44201)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"ALL\",\n",
      "        \"value_represented\": \"Select all Parameters Available\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"AQI POLLUTANTS\",\n",
      "        \"value_represented\": \"Pollutants that have an AQI Defined\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CORE_HAPS\",\n",
      "        \"value_represented\": \"Urban Air Toxic Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CRITERIA\",\n",
      "        \"value_represented\": \"Criteria Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CSN DART\",\n",
      "        \"value_represented\": \"List of CSN speciation parameters to populate the STI DART tool\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"FORECAST\",\n",
      "        \"value_represented\": \"Parameters routinely extracted by AirNow (STI)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"HAPS\",\n",
      "        \"value_represented\": \"Hazardous Air Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"IMPROVE CARBON\",\n",
      "        \"value_represented\": \"IMPROVE Carbon Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"IMPROVE_SPECIATION\",\n",
      "        \"value_represented\": \"PM2.5 Speciated Parameters Measured at IMPROVE sites\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"MET\",\n",
      "        \"value_represented\": \"Meteorological Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"NATTS CORE HAPS\",\n",
      "        \"value_represented\": \"The core list of toxics of interest to the NATTS program.\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"NATTS REQUIRED\",\n",
      "        \"value_represented\": \"Required compounds to be collected in the National Air Toxics Network\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PAMS\",\n",
      "        \"value_represented\": \"Photochemical Assessment Monitoring System\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PAMS_VOC\",\n",
      "        \"value_represented\": \"Volatile Organic Compound subset of the PAMS Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM COARSE\",\n",
      "        \"value_represented\": \"PM between 2.5 and 10 micrometers\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM10 SPECIATION\",\n",
      "        \"value_represented\": \"PM10 Speciated Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM2.5 CONT NONREF\",\n",
      "        \"value_represented\": \"PM2.5 Continuous, Nonreference Methods\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM2.5 MASS/QA\",\n",
      "        \"value_represented\": \"PM2.5 Mass and QA Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SCHOOL AIR TOXICS\",\n",
      "        \"value_represented\": \"School Air Toxics Program Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION\",\n",
      "        \"value_represented\": \"PM2.5 Speciated Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION CARBON\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Carbon Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION CATION/ANION\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Cation/Anion Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION METALS\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Metal Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"UATMP CARBONYL\",\n",
      "        \"value_represented\": \"Urban Air Toxics Monitoring Program Carbonyls\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"UATMP VOC\",\n",
      "        \"value_represented\": \"Urban Air Toxics Monitoring Program VOCs\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"VOC\",\n",
      "        \"value_represented\": \"Volatile organic compounds\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# create a copy of the AQS_REQUEST_TEMPLATE and populate it with your email and API key\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "\n",
    "# send a request to retrieve a list of information based on the populated request_data\n",
    "response = request_list_info(request_template=request_data)\n",
    "\n",
    "# check if the response status is \"Success\"\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    # print the data in a nicely formatted JSON format\n",
    "    print(json.dumps(response['Data'], indent=4))\n",
    "else:\n",
    "    # print the entire response if it's not a success\n",
    "    print(json.dumps(response, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the quest for accurate air quality monitoring, the focus on specific sensors called AQI Pollutants is vital. These sensors are designed to measure key air quality indicators, including PM2.5, PM10, VOCs, CO, NO2, and other pollutants. The resulting response typically includes a comprehensive list of sensor ID numbers, sensor names, and detailed descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AQI_PARAM_CLASS = \"AQI POLLUTANTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"42101\",\n",
      "        \"value_represented\": \"Carbon monoxide\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"42401\",\n",
      "        \"value_represented\": \"Sulfur dioxide\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"42602\",\n",
      "        \"value_represented\": \"Nitrogen dioxide (NO2)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"44201\",\n",
      "        \"value_represented\": \"Ozone\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"81102\",\n",
      "        \"value_represented\": \"PM10 Total 0-10um STP\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"88101\",\n",
      "        \"value_represented\": \"PM2.5 - Local Conditions\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"88502\",\n",
      "        \"value_represented\": \"Acceptable PM2.5 AQI & Speciation Mass\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#   Structure a request to get the sensor IDs associated with the AQI\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['pclass'] = AQI_PARAM_CLASS  # here we specify that we want this 'pclass' or parameter classs\n",
    "\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_PARAMS)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaseous AQI pollutants CO, SO2, NO2, and O2\n",
    "AQI_PARAMS_GASEOUS = \"42101,42401,42602,44201\"\n",
    "\n",
    "# particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- defining the city ---------------------------- #\n",
    "\n",
    "CITY_LOCATIONS = {\n",
    "    'muskogee' :       {'city'   : 'Muskogee',\n",
    "                       'county' : 'Muskogee',\n",
    "                       'state'  : 'Oklahoma',\n",
    "                       'fips'   : '40101',\n",
    "                       'latlon' : [35.7479, -95.3697] },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"0160\",\n",
      "        \"value_represented\": \"5 MILES SOUTH OF HASKELL AT OSU RESEARCH STATION\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0161\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0162\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0163\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0164\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0166\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0167\",\n",
      "        \"value_represented\": \"MUSKOGEE WATER TREATMENT PLANT\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0168\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0169\",\n",
      "        \"value_represented\": \"DOWNTOWN MUSKOGEE\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0170\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"9019\",\n",
      "        \"value_represented\": null\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ nearby stations ----------------------------- #\n",
    "\n",
    "#\n",
    "#  This list request should give us a list of all the monitoring stations in the county specified by the\n",
    "#  given city selected from the CITY_LOCATIONS dictionary\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['state'] = CITY_LOCATIONS['muskogee']['fips'][:2]   # the first two digits (characters) of FIPS is the state code\n",
    "request_data['county'] = CITY_LOCATIONS['muskogee']['fips'][2:]  # the last three digits (characters) of FIPS is the county code\n",
    "\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_SITES)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- restructing response --------------------------- #\n",
    "\n",
    "def request_daily_summary(email_address = None, key = None, param=None,\n",
    "                          begin_date = None, end_date = None, fips = None,\n",
    "                          endpoint_url = API_REQUEST_URL,\n",
    "                          endpoint_action = API_ACTION_DAILY_SUMMARY_COUNTY,\n",
    "                          request_template = AQS_REQUEST_TEMPLATE,\n",
    "                          headers = None):\n",
    "\n",
    "    #  this prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]\n",
    "\n",
    "    # make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_daily_summary()'\")\n",
    "    if not request_template['key']:\n",
    "        raise Exception(\"Must supply a key to call 'request_daily_summary()'\")\n",
    "    if not request_template['param']:\n",
    "        raise Exception(\"Must supply param values to call 'request_daily_summary()'\")\n",
    "    if not request_template['begin_date']:\n",
    "        raise Exception(\"Must supply a begin_date to call 'request_daily_summary()'\")\n",
    "    if not request_template['end_date']:\n",
    "        raise Exception(\"Must supply an end_date to call 'request_daily_summary()'\")\n",
    "    # note we're not validating FIPS fields because not all of the daily summary actions require the FIPS numbers\n",
    "\n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['param'] = AQI_PARAMS_GASEOUS\n",
    "request_data['state'] = CITY_LOCATIONS['muskogee']['fips'][:2]\n",
    "request_data['county'] = CITY_LOCATIONS['muskogee']['fips'][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_FIELDS = ['sample_duration','observation_count','arithmetic_mean','aqi']\n",
    "\n",
    "#    The function creates a summary record\n",
    "def extract_summary_from_response(r=None, fields=EXTRACTION_FIELDS):\n",
    "    # the result will be structured around monitoring site, parameter, and then date\n",
    "    result = dict()\n",
    "    data = r[\"Data\"]\n",
    "    for record in data:\n",
    "\n",
    "        # make sure the record is set up\n",
    "        site = record['site_number']\n",
    "        param = record['parameter_code']\n",
    "\n",
    "        #date = record['date_local']    # this version keeps the respnse value YYYY-\n",
    "        date = record['date_local'].replace('-','') # this puts it in YYYYMMDD format\n",
    "        if site not in result:\n",
    "            result[site] = dict()\n",
    "            result[site]['local_site_name'] = record['local_site_name']\n",
    "            result[site]['site_address'] = record['site_address']\n",
    "            result[site]['state'] = record['state']\n",
    "            result[site]['county'] = record['county']\n",
    "            result[site]['city'] = record['city']\n",
    "            result[site]['pollutant_type'] = dict()\n",
    "        if param not in result[site]['pollutant_type']:\n",
    "            result[site]['pollutant_type'][param] = dict()\n",
    "            result[site]['pollutant_type'][param]['parameter_name'] = record['parameter']\n",
    "            result[site]['pollutant_type'][param]['units_of_measure'] = record['units_of_measure']\n",
    "            result[site]['pollutant_type'][param]['method'] = record['method']\n",
    "            result[site]['pollutant_type'][param]['data'] = dict()\n",
    "        if date not in result[site]['pollutant_type'][param]['data']:\n",
    "            result[site]['pollutant_type'][param]['data'][date] = list()\n",
    "\n",
    "        # now extract the specified fields\n",
    "        extract = dict()\n",
    "        for k in fields:\n",
    "            if str(k) in record:\n",
    "                extract[str(k)] = record[k]\n",
    "            else:\n",
    "                # this makes sure we always have the requested fields, even if\n",
    "                # we have a missing value for a given day/month\n",
    "                extract[str(k)] = None\n",
    "\n",
    "        # add this extraction to the list for the day\n",
    "        result[site]['pollutant_type'][param]['data'][date].append(extract)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19630501-19630531.\n",
      "No data for 19630701-19630731.\n",
      "No data for 19630801-19630831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [07:33<7:25:46, 453.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19631001-19631031.\n",
      "No data for 19640501-19640531.\n",
      "No data for 19640701-19640731.\n",
      "No data for 19640801-19640831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/60 [15:07<7:18:37, 453.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19641001-19641031.\n",
      "No data for 19650501-19650531.\n",
      "No data for 19650701-19650731.\n",
      "No data for 19650801-19650831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/60 [22:42<7:11:41, 454.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19651001-19651031.\n",
      "No data for 19660501-19660531.\n",
      "No data for 19660701-19660731.\n",
      "No data for 19660801-19660831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/60 [30:16<7:03:52, 454.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19661001-19661031.\n",
      "No data for 19670501-19670531.\n",
      "No data for 19670701-19670731.\n",
      "No data for 19670801-19670831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/60 [37:50<6:56:13, 454.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19671001-19671031.\n",
      "No data for 19680501-19680531.\n",
      "No data for 19680701-19680731.\n",
      "No data for 19680801-19680831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6/60 [45:23<6:48:30, 453.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19681001-19681031.\n",
      "No data for 19690501-19690531.\n",
      "No data for 19690701-19690731.\n",
      "No data for 19690801-19690831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/60 [52:58<6:41:03, 454.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19691001-19691031.\n",
      "No data for 19700501-19700531.\n",
      "No data for 19700701-19700731.\n",
      "No data for 19700801-19700831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 8/60 [1:00:32<6:33:33, 454.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19701001-19701031.\n",
      "No data for 19710501-19710531.\n",
      "No data for 19710701-19710731.\n",
      "No data for 19710801-19710831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/60 [1:08:06<6:25:54, 454.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19711001-19711031.\n",
      "No data for 19720501-19720531.\n",
      "No data for 19720701-19720731.\n",
      "No data for 19720801-19720831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10/60 [1:15:39<6:18:10, 453.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19721001-19721031.\n",
      "No data for 19730501-19730531.\n",
      "No data for 19730701-19730731.\n",
      "No data for 19730801-19730831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [1:23:12<6:10:30, 453.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19731001-19731031.\n",
      "No data for 19740501-19740531.\n",
      "No data for 19740701-19740731.\n",
      "No data for 19740801-19740831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12/60 [1:30:46<6:02:53, 453.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19741001-19741031.\n",
      "No data for 19750501-19750531.\n",
      "No data for 19750701-19750731.\n",
      "No data for 19750801-19750831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13/60 [1:38:21<5:55:36, 453.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19751001-19751031.\n",
      "No data for 19760501-19760531.\n",
      "No data for 19760701-19760731.\n",
      "No data for 19760801-19760831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14/60 [1:45:54<5:47:58, 453.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19761001-19761031.\n",
      "No data for 19770501-19770531.\n",
      "No data for 19770701-19770731.\n",
      "No data for 19770801-19770831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15/60 [1:53:28<5:40:27, 453.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19771001-19771031.\n",
      "No data for 19780501-19780531.\n",
      "No data for 19780701-19780731.\n",
      "No data for 19780801-19780831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 16/60 [2:05:32<6:32:21, 535.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19781001-19781031.\n",
      "No data for 19790501-19790531.\n",
      "No data for 19790701-19790731.\n",
      "No data for 19790801-19790831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 17/60 [2:41:12<12:09:27, 1017.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19791001-19791031.\n",
      "No data for 19800501-19800531.\n",
      "No data for 19800701-19800731.\n",
      "No data for 19800801-19800831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 18/60 [2:48:46<9:53:44, 848.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19801001-19801031.\n",
      "No data for 19810501-19810531.\n",
      "No data for 19810701-19810731.\n",
      "No data for 19810801-19810831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 19/60 [2:56:19<8:18:35, 729.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19811001-19811031.\n",
      "No data for 19820501-19820531.\n",
      "No data for 19820701-19820731.\n",
      "No data for 19820801-19820831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 20/60 [3:03:54<7:11:28, 647.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19821001-19821031.\n",
      "No data for 19830501-19830531.\n",
      "No data for 19830701-19830731.\n",
      "No data for 19830801-19830831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [3:11:28<6:22:58, 589.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19831001-19831031.\n",
      "No data for 19840501-19840531.\n",
      "No data for 19840701-19840731.\n",
      "No data for 19840801-19840831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22/60 [3:19:02<5:47:20, 548.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19841001-19841031.\n",
      "No data for 19850501-19850531.\n",
      "No data for 19850701-19850731.\n",
      "No data for 19850801-19850831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 23/60 [3:26:35<5:20:38, 519.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19851001-19851031.\n",
      "No data for 19860501-19860531.\n",
      "No data for 19860701-19860731.\n",
      "No data for 19860801-19860831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24/60 [3:34:09<5:00:05, 500.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19861001-19861031.\n",
      "No data for 19870501-19870531.\n",
      "No data for 19870701-19870731.\n",
      "No data for 19870801-19870831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 25/60 [3:41:43<4:43:36, 486.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19871001-19871031.\n",
      "No data for 19880501-19880531.\n",
      "No data for 19880701-19880731.\n",
      "No data for 19880801-19880831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 26/60 [3:49:16<4:29:52, 476.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19881001-19881031.\n",
      "No data for 19890501-19890531.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 35/60 [4:57:15<3:09:22, 454.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19980501-19980531.\n",
      "No data for 19980701-19980731.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 36/60 [5:04:48<3:01:36, 454.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 19981001-19981031.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 56/60 [7:35:57<30:12, 453.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 20190701-20190731.\n",
      "No data for 20190801-20190831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 57/60 [7:43:30<22:39, 453.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 20191001-20191031.\n",
      "No data for 20200501-20200531.\n",
      "No data for 20200701-20200731.\n",
      "No data for 20200801-20200831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 58/60 [7:51:04<15:06, 453.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 20201001-20201031.\n",
      "No data for 20210501-20210531.\n",
      "No data for 20210701-20210731.\n",
      "No data for 20210801-20210831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 59/60 [7:58:37<07:33, 453.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 20211001-20211031.\n",
      "No data for 20220501-20220531.\n",
      "No data for 20220701-20220731.\n",
      "No data for 20220801-20220831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [8:06:09<00:00, 486.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 20221001-20221031.\n",
      "{1989: 30.94736842105263, 1990: 25.466666666666665, 1991: 36.0, 1992: 30.94736842105263, 1993: 34.666666666666664, 1994: 28.15, 1995: 34.4, 1996: 29.05263157894737, 1997: 35.095238095238095, 1998: 39.6, 1999: 50.142857142857146, 2000: 45.111111111111114, 2001: 36.61904761904762, 2002: 24.848, 2003: 26.83606557377049, 2004: 22.008333333333333, 2005: 27.97457627118644, 2006: 25.76271186440678, 2007: 26.443548387096776, 2008: 24.225806451612904, 2009: 20.116666666666667, 2010: 30.008064516129032, 2011: 25.530434782608694, 2012: 27.30081300813008, 2013: 22.422535211267604, 2014: 24.705882352941178, 2015: 23.661971830985916, 2016: 20.177419354838708, 2017: 22.458333333333332, 2018: 25.115702479338843, 2019: 11.571428571428571}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Request daily summary data for the years 1963-2023\n",
    "average_aqi_per_year = {}\n",
    "\n",
    "# Define the months from May to October\n",
    "target_months = [5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for year in tqdm(range(1963, 2023)):\n",
    "    year_aqi_count = 0\n",
    "    year_aqi_sum = 0\n",
    "\n",
    "    for month in target_months:\n",
    "        begin_date = f\"{year}{month:02d}01\"\n",
    "        end_date = f\"{year}{month:02d}31\"\n",
    "        request_data['param'] = AQI_PARAMS_PARTICULATES\n",
    "\n",
    "        # request daily summary data for the specified month\n",
    "        particulate_aqi = request_daily_summary(request_template=request_data, begin_date=begin_date, end_date=end_date)\n",
    "\n",
    "        try:\n",
    "            if particulate_aqi[\"Header\"][0]['status'].startswith(\"No data \"):\n",
    "                print(f\"No data for {begin_date}-{end_date}.\")\n",
    "            extract_particulate = extract_summary_from_response(particulate_aqi)\n",
    "            first_site_location = next(iter(extract_particulate.values()))\n",
    "            data_for_first_site = first_site_location.get('pollutant_type', {})\n",
    "\n",
    "            for pollutant_data in data_for_first_site.values():\n",
    "                year_aqi_data = pollutant_data.get('data', {})\n",
    "\n",
    "                # Loop through the data for each date in the month\n",
    "                for date, aqi_list in year_aqi_data.items():\n",
    "                    for entry in aqi_list:\n",
    "                        if entry['aqi']:\n",
    "                            year_aqi_sum += entry['aqi']\n",
    "                            year_aqi_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Calculate the average AQI for the year\n",
    "    if year_aqi_count > 0:\n",
    "        average_aqi = year_aqi_sum / year_aqi_count\n",
    "        average_aqi_per_year[year] = average_aqi\n",
    "\n",
    "# Print or use the average_aqi_per_year dictionary as needed\n",
    "print(average_aqi_per_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Avg_AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1989</td>\n",
       "      <td>30.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>25.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992</td>\n",
       "      <td>30.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993</td>\n",
       "      <td>34.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year    Avg_AQI\n",
       "0  1989  30.947368\n",
       "1  1990  25.466667\n",
       "2  1991  36.000000\n",
       "3  1992  30.947368\n",
       "4  1993  34.666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(average_aqi_per_year.items()), columns=['Year', 'Avg_AQI'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"avg-aqi-yearly-fire-season.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
